# Предсказание зарплат по данным hh.ru (Линейная регрессия)

Модель линейной регрессии для предсказания зарплат на основе обработанных данных резюме с hh.ru.

## Структура проекта
assignment2_regression/\
├── app.py&emsp;&emsp;&emsp;&emsp;# Предсказание: python app.py x_data.npy\
├── train.py&emsp;&emsp;&emsp;&ensp;# Обучение модели (выполняется 1 раз)\
├── model.py&emsp;&emsp;&emsp;# Реализация линейной регрессии\
├── requirements.txt\
├── README.md\
├── .gitignore\
└── resources/&emsp;&emsp;&emsp;# Сохранённые веса модели\
&emsp;&emsp;├── weights.npy\
&emsp;&emsp;└── bias.npy

## Установка
```bash
pip install -r requirements.txt
```
## Обучение модели (выполнить 1 раз)
python train.py

Скрипт автоматически:
1. Загрузит x_data.npy и y_data.npy из папки ./assignment1_preprocessing/
2. Обучит модель линейной регрессии
3. Сохранит веса в папку resources/

## Использование
python app.py путь/к/x_data.npy

Вывод: список зарплат в рублях, по одной на строку:
65432.10
78901.23
45678.90
...

## Реализация
- Чистая линейная регрессия без сторонних ML-библиотек (только numpy)
- Обучение через нормальное уравнение: θ = (X^T X)^(-1) X^T y
- Веса сохраняются вручную для прозрачности и соответствия требованиям задания
- Соответствует интерфейсу: python app path/to/x_data.npy

## Выводы по качеству линейной регрессии:

1. Наблюдаемые метрики после фильтрации выбросов:
   • R² = 0.208 (20.8% объяснённой дисперсии)
   • RMSE = 62 451 руб. (средняя ошибка предсказания)
   • Отфильтровано 2 319 выбросов (3.5% от общего объёма данных)

2. Причины улучшения качества:
   а) Удаление аномальных значений:
      — Зарплаты-заглушки («1 рубль») — распространённая практика на hh.ru
        для скрытия реальной зарплаты
      — Экстремальные значения (> 1 000 000 руб.) — ошибки ввода или нетипичные кейсы
   б) Повышение однородности данных:
      — После удаления выбросов распределение зарплат стало ближе к нормальному
      — Линейная регрессия лучше работает на «чистых» данных без экстремумов

3. Ограничения модели (почему R² не 0.8+):
   а) Гетерогенность профессий:
      — Датасет содержит смешанные профессии (администраторы, инженеры, аналитики)
      — Зарплата зависит от профессии, а не только от возраста/опыта/города
   б) Ограниченное признаковое пространство:
      — Пайплайн задания №1 извлекает только 3 признака: возраст, опыт, город
      — Отсутствуют критически важные признаки: должность, ключевые навыки, стек технологий
   в) Нелинейные зависимости:
      — Связь между опытом и зарплатой нелинейна (логарифмическая)
      — Линейная регрессия не может уловить такие зависимости без полиномиальных признаков

4. Возможности дальнейшего улучшения:
   • Фильтрация ТОЛЬКО разработчиков (как в задании №3) повысила бы однородность данных
   • Добавление полиномиальных признаков (опыт², опыт³) для захвата нелинейности
   • Расширение признакового пространства за счёт парсинга должности и навыков

5. Заключение:
   Модель демонстрирует умеренное качество предсказания (R² = 0.208), что приемлемо
   для учебного проекта с ограниченным набором признаков. Фильтрация выбросов
   позволила значительно улучшить метрики (с 3.4% до 20.8%), что подтверждает
   важность этапа очистки данных в пайплайне машинного обучения. Несмотря на
   ограничения входных данных, реализованный пайплайн соответствует требованиям ТЗ
   и демонстрирует корректную работу линейной регрессии на реальных данных.